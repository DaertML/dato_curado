import re
import os
import json 
from ollama import chat
from c_compiler_bridge import CCompilerBridge

# --- LLM Configuration ---
# NOTE: Ensure the Ollama server is running and this model is pulled locally.
OLLAMA_MODEL = "qwen3:8b" 

prompt_defense_generation = """
You are a static analysis security expert specializing in C memory safety.
Your task is to review a provided vulnerable C code snippet, identify the exact vulnerability, and propose a specific, corrected, secure version of the code.

Follow these steps:
1. **Identify the Vulnerability (ANALYSIS_VULNERABILITY):** State the type of flaw (e.g., Stack Buffer Overflow, Use After Free) and the specific line/function causing it.
2. **Propose the Fix (SECURE_CODE_FIX):** Provide the complete, corrected C code that eliminates the vulnerability, including all necessary includes and main function. Ensure the fixed code is fully runnable and uses secure functions like strncpy, proper length checks, or explicit NULL checks after free.
3. **Explain the Defense (DEFENSE_EXPLANATION):** Explain *why* the fix works and the principle of secure coding it enforces.
4. **Write the code of a Exploit (EXPLOIT_GENERATION):** *Only* Write the C code that when compiled exploits the vulnerability present in the code.

Use the following exact delimiters:
[ANALYSIS_VULNERABILITY]
...
[END_ANALYSIS_VULNERABILITY]

[SECURE_CODE_FIX]
...
[END_SECURE_CODE_FIX]

[DEFENSE_EXPLANATION]
...
[END_DEFENSE_EXPLANATION]

[EXPLOIT_GENERATION]
...
[END_EXPLOIT_GENERATION]
"""

# --- Core Utilities ---

def run_ollama(model: str, prompt: str, question_code: str) -> str:
    """Makes a real call to the Ollama server."""
    print(f"\n--- OLLAMA CALL ({model}) ---")
    
    # Wrap the C code in Markdown for clarity in the prompt
    user_query = f"Analyze and fix this vulnerable C code:\n\n```c\n{question_code}\n```"
    
    try:
        response = chat(model=model, messages=[
            {
                'role': 'system',
                'content': prompt,
            },
            {
                'role': 'user',
                'content': user_query
            }
        ])
        
        # The actual response structure is complex, but the content is here:
        return response['message']['content']
    except Exception as e:
        print(f"ERROR: Failed to connect to Ollama server or receive response: {e}")
        return ""


def extract_security_components(response: str) -> tuple[str, str, str, str]:
    """
    Extracts the analysis, secure code, and defense explanation from the LLM response.
    """
    
    # Use re.DOTALL to match across newlines and allow for surrounding whitespace
    analysis_match = re.search(r"\[ANALYSIS_VULNERABILITY\]\s*(.*?)\s*\[END_ANALYSIS_VULNERABILITY\]", response, re.DOTALL)
    code_match = re.search(r"\[SECURE_CODE_FIX\]\s*(.*?)\s*\[END_SECURE_CODE_FIX\]", response, re.DOTALL)
    defense_match = re.search(r"\[DEFENSE_EXPLANATION\]\s*(.*?)\s*\[END_DEFENSE_EXPLANATION\]", response, re.DOTALL)
    exploit_match = re.search(r"\[EXPLOIT_GENERATION\]\s*(.*?)\s*\[END_EXPLOIT_GENERATION\]", response, re.DOTALL)

    # Extract the content, stripping outer whitespace
    analysis = analysis_match.group(1).strip() if analysis_match else ""
    secure_code = code_match.group(1).strip() if code_match else ""
    defense = defense_match.group(1).strip() if defense_match else ""
    exploit = exploit_match.group(1).strip() if exploit_match else ""

    # Optional: Clean up code fences if the LLM wrapped the code in ```c ... ```
    code_content = re.sub(r'```c\n|```', '', secure_code).strip()
    
    return analysis, code_content, defense, exploit, response # Return full response for debugging


def check_security_fix(compiler: CCompilerBridge, question_data: dict) -> dict:
    """
    1. Calls the LLM to generate the security analysis and fix.
    2. Calls the C Compiler (real) to verify the fixed code.
    3. Returns the result.
    """
    
    # 1. Generate security analysis and fixed code from the vulnerable code
    llm_response = run_ollama(OLLAMA_MODEL, prompt_defense_generation, question_data['question'])
    
    analysis, secure_code, defense, exploit, full_llm_response = extract_security_components(llm_response)
    
    print("-" * 50)
    print(f"Vulnerable Code: {question_data['question'][:100]}...")
    
    if not analysis or not secure_code or not defense:
        print("!! ERROR: Failed to extract complete security components from LLM.")
        return {
            "vulnerable_code": question_data['question'],
            "analysis": analysis,
            "secure_code": secure_code,
            "defense": defense,
            "compiles_and_runs": False,
            "error": "Parsing failed or missing component",
            "exploit": exploit,
            "llm_output": full_llm_response 
        }
        
    # 2. Check validity of the proposed fix using the real Compiler Bridge
    is_valid = compiler.test_secure_code_fix(secure_code)
    is_valid_exploit = compiler.test_secure_code_fix(exploit)
    
    # 3. Return the comprehensive result
    return {
        "vulnerable_code": question_data['question'],
        "analysis": analysis, 
        "secure_code": secure_code,
        "defense": defense,
        "exploit": exploit,
        "compiles_and_runs": is_valid,
        "exploit_compiles_and_runs": is_valid_exploit
    }