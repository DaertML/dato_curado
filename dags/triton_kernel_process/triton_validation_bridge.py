import io
import contextlib
import sys
import torch
import numpy as np
# Attempt to import triton and tl globally for the exec scope
try:
    import triton
    import triton.language as tl
except ImportError as e:
    print(f"CRITICAL ERROR: Failed to import Triton. Ensure it is installed. Details: {e}")
    # Define placeholder classes/modules to allow the rest of the code to run structurally
    triton = None
    tl = None
    
# We will now use TritonExecutionBridge, renamed from TritonValidationBridge

class TritonExecutionBridge:
    """
    Executes the generated Triton kernel and Python test code using a dynamic scope.
    """

    def __init__(self):
        # CRITICAL FIX 2: Check for essential dependencies (torch/cuda) at startup
        if not torch.cuda.is_available():
            print("ðŸš¨ WARNING: CUDA not available. Triton execution will fail!")
        if triton is None:
            print("ðŸš¨ WARNING: Triton not imported. Execution will fail!")
            
        self.exec_globals = {
            'torch': torch,
            'np': np,
            'triton': triton,
            'tl': tl,
            # Add other necessary functions/modules here if the LLM uses them
        }
        print("Triton Execution Bridge initialized. Dependencies loaded into execution scope.")


    def execute_and_validate(self, triton_kernel_code: str, python_test_code: str) -> tuple[bool, str]:
        """
        Dynamically executes the combined kernel and test code.
        """
        
        # 1. Combine code (with local imports stripped as they are now in exec_globals)
        # Note: We must ensure the LLM's code uses 'triton'/'tl' directly, which the prompt requires.
        full_executable_code = f"{triton_kernel_code}\n\n{python_test_code}"
        
        # 2. Dynamic Execution using `exec`
        captured_output = io.StringIO()
        
        # CRITICAL FIX 3: Use a fresh scope dictionary for each execution to prevent
        # cross-contamination between different kernel runs.
        exec_locals = {}
        exec_scope = self.exec_globals.copy() 

        # We need to execute the full code so the kernel is defined in a scope accessible
        # by the test code.

        try:
            # Execute the code, capturing stdout/stderr
            with contextlib.redirect_stdout(captured_output), contextlib.redirect_stderr(captured_output):
                # We execute the entire code block in the global scope augmented with necessary imports.
                # The execution will define the kernel function and then call check_correctness().
                exec(full_executable_code, exec_scope, exec_locals)
            
            output = captured_output.getvalue()
            
            # 3. Validation Check
            # Check for the success message generated by the unit test
            if "Test passed" in output:
                print("\n[TRITON EXECUTION] Output: 'Kernel executed and verified successfully against PyTorch/NumPy.'")
                return True, ""
            
            # If the code ran but the success message wasn't found, it likely failed an internal check.
            return False, f"Execution Failure: Test executed but did not report success. Output:\n{output}"

        except Exception as e:
            # Capture any runtime exceptions
            error_output = captured_output.getvalue()
            # Clean up the error message for better readability
            error_lines = str(e).split('\n')
            
            full_error_details = f"RUNTIME EXCEPTION: {type(e).__name__}: {str(e)}\n--- Captured Output ---\n{error_output}"
            print(f"\n[TRITON EXECUTION] Output: 'RUNTIME EXCEPTION/FAILURE.'")
            print(f"Error summary: {error_lines[0]}") 
            
            # Example of the error for Problem 5:
            # The LLM wrote: fused_comparison[A.size(0),](A, B, C, A.size(0))
            # This is invalid indexing/launch syntax. The fix is to ensure the LLM is corrected 
            # via prompt (or fixed here, but fixing here defeats the LLM evaluation purpose).
            
            return False, full_error_details